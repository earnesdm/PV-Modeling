# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yl-ktv6REDMdt-bMk18A5uM4_Lnp7GIS
"""

import torch
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data import sampler
import torchvision.datasets as dset
import torchvision.transforms as T
import numpy as np

# Bidirectional recurrent neural network (many-to-one)
class BiRNN_2(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(BiRNN_2, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm_1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)
        #self.lstm_2 = nn.LSTM(128, 1, 1, batch_first=True, bidirectional=False)
        self.lstm_2 = nn.LSTM(2*hidden_size, 1, 1, batch_first=True, bidirectional=True)
    
    def forward(self, x): 
        x = torch.transpose(x, 2, 1)
        # x dim: [batch size, sentence length, feature dim]    
      
        out, _ = self.lstm_1(x)  # out: tensor of shape (batch_size, seq_length, hidden_size*2)
        # output dim: [batch size, sentence length, hidden dim*2]
        # hidden dim: [2, batch size, hidden dim]

        out, _ = self.lstm_2(out)

        # average pooling
        out = (out[:, :, 0] + out[:, :, 1])/2

        return out.squeeze()